{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Walt Whitman\n",
      "\n",
      "O Captain! my Captain! our fearful trip is done,\n",
      "The ship has weather’d every rack, the prize we sought is won,\n",
      "The port is near, the bells I hear, the people all exulting,\n",
      "While follow eyes the steady keel, the vessel grim and daring;\n",
      "\n",
      "But O heart! heart! heart!\n",
      "O the bleeding drops of red,\n",
      "Where on the deck my Captain lies,\n",
      "Fallen cold and dead.\n",
      "\n",
      "O Captain! my Captain! rise up and hear the bells;\n",
      "Rise up—for you the flag is flung—for you the bugle trills,\n",
      "For you bouquets and ribbon’d wreaths—for you the shores a-crowding,\n",
      "For you they call, the swaying mass, their eager faces turning;\n",
      "\n",
      "Here Captain! dear father!\n",
      "This arm beneath your head!\n",
      "It is some dream that on the deck,\n",
      "You’ve fallen cold and dead.\n",
      "\n",
      "My Captain does not answer, his lips are pale and still,\n",
      "My father does not feel my arm, he has no pulse nor will,\n",
      "The ship is anchor’d safe and sound, its voyage closed and done,\n",
      "From fearful trip the victor ship comes in with object won;\n",
      "\n",
      "Exult, O shores, and ring, O bells!\n",
      "But I, with mournful tread,\n",
      "Walk the deck my Captain lies,\n",
      "Fallen cold and dead.\n"
     ]
    }
   ],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=\"<api_key>\",  # Use Azure API key\n",
    "  azure_endpoint=\"<azure_openai_endpoint>\", # Use Azure endpoint\n",
    "  api_version=\"2023-10-01-preview\",\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "## engine is the name of your deployment\n",
    "def get_completion(prompt, engine=\"gpt35\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "O captain! my captain!\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesson Plan: Generative AI\n",
      "\n",
      "Lesson 1: Introduction to Generative AI\n",
      "- Overview of Generative AI and its applications\n",
      "- Understanding the difference between generative and discriminative models\n",
      "- Introduction to popular generative AI techniques such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)\n",
      "\n",
      "Lesson 2: Variational Autoencoders (VAEs)\n",
      "- Understanding the architecture and working principles of VAEs\n",
      "- Training a VAE model using a dataset\n",
      "- Generating new samples using the trained VAE model\n",
      "- Evaluating the quality of generated samples\n",
      "\n",
      "Lesson 3: Generative Adversarial Networks (GANs)\n",
      "- Understanding the architecture and working principles of GANs\n",
      "- Training a GAN model using a dataset\n",
      "- Generating new samples using the trained GAN model\n",
      "- Evaluating the quality of generated samples and the stability of the GAN model\n",
      "\n",
      "Lesson 4: Conditional Generative Models\n",
      "- Introduction to conditional generative models\n",
      "- Training a conditional VAE or GAN model using labeled data\n",
      "- Generating new samples conditioned on specific attributes or labels\n",
      "\n",
      "Lesson 5: Advanced Topics in Generative AI\n",
      "- Exploring advanced generative AI techniques such as StyleGAN and Transformer-based models\n",
      "- Understanding the challenges and limitations of generative AI\n",
      "- Discussing real-world applications of generative AI in fields like art, music, and text generation\n",
      "\n",
      "Lesson 6: Ethical Considerations in Generative AI\n",
      "- Discussing ethical concerns related to generative AI, such as deepfakes and misinformation\n",
      "- Exploring ways to mitigate potential risks and ensure responsible use of generative AI\n",
      "\n",
      "Lesson 7: Hands-on Project\n",
      "- Students will work on a hands-on project to apply their knowledge of generative AI\n",
      "- They will choose a specific application or problem domain and develop a generative AI model to solve it\n",
      "- Students will present their projects and discuss the challenges faced and lessons learned\n",
      "\n",
      "Lesson 8: Review and Recap\n",
      "- Reviewing key concepts and techniques covered throughout the course\n",
      "- Addressing any remaining questions or concerns\n",
      "- Providing additional resources for further exploration in generative AI\n",
      "\n",
      "Note: The lesson plan can be adjusted based on the duration and level of the course. Additional lessons or topics can be added as needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan for Generative AI.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do you think it was played? It was actually played in Arlington, Texas at the Globe Life Field.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt35\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
